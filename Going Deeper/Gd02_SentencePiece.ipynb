{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1192ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'nsmc' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from konlpy.tag import Mecab\n",
    "from collections import Counter\n",
    "\n",
    "!git clone https://github.com/e9t/nsmc.git\n",
    "os.listdir('nsmc')\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d10b34a",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea2b9e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_table('nsmc/'+'ratings_train.txt')\n",
    "test_data = pd.read_table('nsmc/'+'ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489939a",
   "metadata": {},
   "source": [
    "## 중복값 제거, 결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd4ce04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터 총량 : 150000\n",
      "학습데이터 고유값 : 146182\n",
      "테스트데이터 총량 : 50000\n",
      "테스트데이터 고유값 : 49157\n",
      "라벨데이터 고유값 : 2\n"
     ]
    }
   ],
   "source": [
    "print(\"학습데이터 총량 :\", len(train_data['document']))\n",
    "print(\"학습데이터 고유값 :\",train_data['document'].nunique())\n",
    "print(\"테스트데이터 총량 :\", len(test_data['document']))\n",
    "print(\"테스트데이터 고유값 :\",test_data['document'].nunique())\n",
    "print(\"라벨데이터 고유값 :\",test_data['label'].nunique()) # label 0: negative / 1: positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff9eaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터 총량 : 146183\n",
      "학습데이터 고유값 : 146182\n",
      "테스트데이터 고유값 : 49157\n",
      "테스트데이터 총량 : 49158\n"
     ]
    }
   ],
   "source": [
    "train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "print(\"학습데이터 총량 :\", len(train_data['document']))\n",
    "print(\"학습데이터 고유값 :\",train_data['document'].nunique())\n",
    "\n",
    "test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "print(\"테스트데이터 고유값 :\",test_data['document'].nunique())\n",
    "print(\"테스트데이터 총량 :\", len(test_data['document']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aa9712f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    1\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f955dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    1\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82b5a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_data.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff81b9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_data.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d574ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    0\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52cc2d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    0\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a0b9c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 146182 entries, 0 to 149999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   id        146182 non-null  int64 \n",
      " 1   document  146182 non-null  object\n",
      " 2   label     146182 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39523a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 49157 entries, 0 to 49999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        49157 non-null  int64 \n",
      " 1   document  49157 non-null  object\n",
      " 2   label     49157 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a32038",
   "metadata": {},
   "source": [
    "## 리뷰 문장 길이 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f47ea47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 33, 17, 29]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_len = list(map(lambda x: len(x), train['document']))\n",
    "doc_len[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89df8216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 146)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_len, max_len = min(doc_len), max(doc_len)\n",
    "min_len, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "581e8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length = np.zeros((max_len), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9165b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sen in train['document']:\n",
    "    sentence_length[len(sen)-1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96b3e589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7414dfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"스폰으로 먹고사는 방송이라 어쩔수 없다고 하지만. 이건 그냥 비현실적인 자동차만;...독일3사&슈퍼카 홍보 프로그램도 아니구.대중적인 자동차 방송으로 이루어 졌으면 합니다. 보는내내 \"\"카탈로그 책자\"\"를 \"\"동영상으로 보여주는 방송\"\" 같아서 씁쓸하네요.!\"',\n",
       " '\"\"\"니 짓은 생각않고, 웬 복수!\"\"의 교훈이라! 그럼 \"\"서바이벌 액션\"\"으로 홍보하면 안되지! 초반 45분은 멋지게 열더니.. 억지 반전, 하드고어로 시간끌다가, 허둥지둥 화해로 끝내버리네. 90분 러닝타임에 엔딩자막만 11분 틀어주는 해괴망측한 영화~!\"',\n",
       " '\"2007.02.25_ 벌교의 한 국밥집_ 점심: \"\"갸는 첫째고, 저 놈은 우리 둘째~\"\" 재문: \"\"아줌마! 미안해~ 그냥.. 아줌마! 나 그 남방 잘 어울려ㅠ_ㅠ?\"\" 대식에게 복수하려던 1주일 전_ 대식의 엄마를 먼저 만났다. 사랑의 꽃남방도..^-^o\"']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 긴 문장\n",
    "list(train[train['document'].apply(lambda x: len(x)==max_len)]['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57f4af53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아', '잼', '1', '4', '굿', '짱']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 짧은 문장\n",
    "list(train[train['document'].apply(lambda x: len(x)==min_len)]['document'])[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "631011ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['최고', '졸작', '대박', '버려']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train[train['document'].apply(lambda x: len(x)==2)]['document'])[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a07df87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  35.981338331668745\n",
      "문장길이 최대 :  146\n",
      "문장길이 표준편차 :  29.49113203149937\n",
      "pad_sequences maxlen :  94\n",
      "전체 문장의 0.9323719746617231%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "doc_len = np.array(doc_len)\n",
    "\n",
    "print('문장길이 평균 : ', np.mean(doc_len))\n",
    "print('문장길이 최대 : ', np.max(doc_len))\n",
    "print('문장길이 표준편차 : ', np.std(doc_len))\n",
    "\n",
    "# 문장 최대 길이를 평균 + 표준편차*2로 설정한다.\n",
    "max_sentence = np.mean(doc_len) + 2 * np.std(doc_len)\n",
    "maxlen = int(max_sentence)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(doc_len < max_sentence) / len(doc_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a5f9ca",
   "metadata": {},
   "source": [
    "두 글자부터는 어느 정도 호불호를 보여주기 때문에 사용할 데이터에 포함한다. 한 글자의 경우 의미 없는 내용인 경우가 많기 때문에 제거한다. 너무 긴 문장은 제거하기 위해 40을 기준으로 두고 중복값도 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2729ac99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7797314</td>\n",
       "      <td>원작의 긴장감을 제대로 살려내지못했다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>6222902</td>\n",
       "      <td>인간이 문제지.. 소는 뭔죄인가..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>8549745</td>\n",
       "      <td>평점이 너무 낮아서...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>9311800</td>\n",
       "      <td>이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>2376369</td>\n",
       "      <td>청춘 영화의 최고봉.방황과 우울했던 날들의 자화상</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>9619869</td>\n",
       "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104874 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                           document  label\n",
       "0        9976970                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1        3819312  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2       10265843                  너무재밓었다그래서보는것을추천한다      0\n",
       "3        9045019      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "6        7797314              원작의 긴장감을 제대로 살려내지못했다.      0\n",
       "...          ...                                ...    ...\n",
       "149995   6222902                인간이 문제지.. 소는 뭔죄인가..      0\n",
       "149996   8549745                      평점이 너무 낮아서...      1\n",
       "149997   9311800    이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?      0\n",
       "149998   2376369        청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n",
       "149999   9619869           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
       "\n",
       "[104874 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_len_cor = 2\n",
    "max_len_cor = 40\n",
    "\n",
    "tr_data = train[train['document'].apply(lambda x: min_len_cor <= len(x) <= max_len_cor)]\n",
    "tr_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "tr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fde54e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6270596</td>\n",
       "      <td>굳 ㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9274899</td>\n",
       "      <td>GDNTOPCLASSINTHECLUB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8544678</td>\n",
       "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6825595</td>\n",
       "      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7898805</td>\n",
       "      <td>음악이 주가 된, 최고의 음악영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49992</th>\n",
       "      <td>1077821</td>\n",
       "      <td>내일 토요일밤 MBC에서 영화 해준다.... 봐야지... 기대</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49993</th>\n",
       "      <td>5494272</td>\n",
       "      <td>액션영화로 기대하지말고 스릴러영화라 생각하고 보면 괜찮은 영화인듯^^</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49994</th>\n",
       "      <td>5567676</td>\n",
       "      <td>정말 너무 재밌음 ㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>9072549</td>\n",
       "      <td>그림도 좋고 완성도도 높았지만... 보는 내내 불안하게 만든다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>6070594</td>\n",
       "      <td>마무리는 또 왜이래</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35375 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                document  label\n",
       "0      6270596                                     굳 ㅋ      1\n",
       "1      9274899                    GDNTOPCLASSINTHECLUB      0\n",
       "2      8544678  뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
       "3      6825595        지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
       "5      7898805                      음악이 주가 된, 최고의 음악영화      1\n",
       "...        ...                                     ...    ...\n",
       "49992  1077821      내일 토요일밤 MBC에서 영화 해준다.... 봐야지... 기대      1\n",
       "49993  5494272  액션영화로 기대하지말고 스릴러영화라 생각하고 보면 괜찮은 영화인듯^^      1\n",
       "49994  5567676                             정말 너무 재밌음 ㅋ      1\n",
       "49997  9072549      그림도 좋고 완성도도 높았지만... 보는 내내 불안하게 만든다      0\n",
       "49999  6070594                              마무리는 또 왜이래      0\n",
       "\n",
       "[35375 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_data = test[test['document'].apply(lambda x: min_len_cor <= len(x) <= max_len_cor)]\n",
    "ts_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "126fc8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아 더빙.. 진짜 짜증나네요 목소리',\n",
       " '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나',\n",
       " '너무재밓었다그래서보는것을추천한다',\n",
       " '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정',\n",
       " '원작의 긴장감을 제대로 살려내지못했다.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [sen for sen in tr_data['document']]\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72f39bac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp --model_prefix=korean_spm --vocab_size=8000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 104874 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=2375584\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1764\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 104874 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 156800 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 104874\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 178356\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 178356 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=83246 obj=15.2936 num_tokens=416252 num_tokens/piece=5.00026\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=77463 obj=14.2716 num_tokens=418908 num_tokens/piece=5.40785\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=58080 obj=14.4201 num_tokens=438360 num_tokens/piece=7.54752\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=57995 obj=14.3535 num_tokens=439142 num_tokens/piece=7.57207\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=43492 obj=14.6648 num_tokens=463925 num_tokens/piece=10.6669\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=43484 obj=14.5891 num_tokens=464008 num_tokens/piece=10.6708\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=32613 obj=14.9221 num_tokens=487935 num_tokens/piece=14.9614\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=32613 obj=14.8456 num_tokens=487932 num_tokens/piece=14.9613\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=24459 obj=15.2182 num_tokens=513554 num_tokens/piece=20.9965\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=24459 obj=15.1408 num_tokens=513569 num_tokens/piece=20.9971\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=18344 obj=15.5531 num_tokens=539832 num_tokens/piece=29.4283\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=18344 obj=15.4731 num_tokens=539848 num_tokens/piece=29.4291\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=13758 obj=15.9048 num_tokens=566790 num_tokens/piece=41.1971\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=13758 obj=15.8189 num_tokens=566838 num_tokens/piece=41.2006\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=10318 obj=16.3037 num_tokens=595977 num_tokens/piece=57.7609\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=10318 obj=16.2061 num_tokens=596143 num_tokens/piece=57.777\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=8800 obj=16.4921 num_tokens=613153 num_tokens/piece=69.6765\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8800 obj=16.4352 num_tokens=613155 num_tokens/piece=69.6767\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 375831 Mar 24 03:22 korean_spm.model\r\n",
      "-rw-r--r-- 1 root root 145189 Mar 24 03:22 korean_spm.vocab\r\n"
     ]
    }
   ],
   "source": [
    "temp_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp'\n",
    "vocab_size = 8000\n",
    "\n",
    "with  open(temp_file, 'w') as f:\n",
    "    for row in corpus:\n",
    "        f.write(str(row) + '\\n')\n",
    "        \n",
    "spm.SentencePieceTrainer.Train('--input={} --model_prefix=korean_spm --vocab_size={}'.format(temp_file, vocab_size))\n",
    "\n",
    "!ls -l korean_spm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2d54a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1668, 10, 409, 16, 1585, 10, 154, 13, 4]\n",
      "['▁아버지', '가', '방', '에', '들어', '가', '신', '다', '.']\n",
      "아버지가방에들어가신다.\n"
     ]
    }
   ],
   "source": [
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('korean_spm.model')\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoding\n",
    "tokensIDs = s.EncodeAsIds('아버지가방에들어가신다.')\n",
    "print(tokensIDs)\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoded pieces\n",
    "print(s.SampleEncodeAsPieces('아버지가방에들어가신다.',1, 0.0))\n",
    "\n",
    "# SentencePiece를 활용한 encoding -> sentence 복원\n",
    "print(s.DecodeIds(tokensIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99fe748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_tokenize(s, corpus):\n",
    "\n",
    "    tensor = []\n",
    "\n",
    "    for sen in corpus:\n",
    "        if type(sen) is not str :\n",
    "            print(\"is not string : {}\".format(sen))\n",
    "            continue\n",
    "        tensor.append(s.EncodeAsIds(sen))\n",
    "\n",
    "    with open(\"./korean_spm.vocab\", 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]\n",
    "\n",
    "        word_index.update({idx:word})\n",
    "        index_word.update({word:idx})\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f59b42b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  47 6204   12   10  150 3300]\n",
      " [1458 5922 3360  809    0    0]\n",
      " [ 909 4633 7991  289 1603    0]]\n"
     ]
    }
   ],
   "source": [
    "my_corpus = ['왜 정확도가 이렇게 나오지','뭐가 잘못된 건지 모르겠다', '누가 알려줬으면 좋겠다']\n",
    "my_tensor, my_word_index, my_index_word = sp_tokenize(s, my_corpus)\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf7b39ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아 더빙.. 진짜 짜증나네요 목소리', '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '너무재밓었다그래서보는것을추천한다', '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정', '원작의 긴장감을 제대로 살려내지못했다.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  45,  906,    5,   28, 2067,   66, 1777,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0],\n",
       "       [1179,    7, 4777,  174, 1161,   29,  265,   40,  163,  507,  369,\n",
       "        1419, 6316,  686,  372],\n",
       "       [  22,  451, 7519,  294, 4688,  641, 2265, 1600,  333,    0,    0,\n",
       "           0,    0,    0,    0],\n",
       "       [2205,   12,  209,  184, 4323,    3,    5, 5590,  975,   95,    5,\n",
       "         654, 5833,    0,    0],\n",
       "       [3146,  712,   14,  671, 3575,  210,   20,  492,  181,    4,    0,\n",
       "           0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor, word_index, index_word = sp_tokenize(s, corpus[:5])\n",
    "print(corpus[0:5])\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "583b4b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_train_word_index, X_train_index_word = sp_tokenize(s, tr_data['document'])\n",
    "X_test,X_test_word_index, X_test_index_word = sp_tokenize(s, ts_data['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58efe702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 list 상태 ==> ndarray로 바꿔주기\n",
    "y_train = np.array(list(tr_data['label']))\n",
    "y_test = np.array(list(ts_data['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "250c3415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터 : 104874\n",
      "타겟데이터 : 104874\n"
     ]
    }
   ],
   "source": [
    "print(\"학습데이터 :\",len(X_train))\n",
    "print(\"타겟데이터 :\",len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8893b293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4976 4976  305   23  180    7  238   10 1996   48  474 1153  241   17\n",
      "    22  949    4    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  30 1951   18   51   78  241 1123   11  113  677 1263  704    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 159   38 1199 2517    9 1223   21   18 4960  203 2333    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]]\n",
      "[[2038   65 3849  287  240 4479  462    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]\n",
      " [2360 4732 5967   31 4338 3583   11 2726  207 1217   18 1522   11  877\n",
      "    92    4    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]\n",
      " [6938   73   22 3102    5   98 4567    5    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_input, val_input, train_target, val_target = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "print(train_input[:3])\n",
    "print(val_input[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c381c44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          128000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 800       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 128,881\n",
      "Trainable params: 128,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 8000    # 어휘 사전의 크기\n",
    "word_vector_dim = 16  # 워드 벡터의 차원 수 (변경 가능한 하이퍼파라미터)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))   \n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9c929ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2622/2622 [==============================] - 59s 22ms/step - loss: 0.3268 - accuracy: 0.8605 - val_loss: 0.3500 - val_accuracy: 0.8466\n",
      "Epoch 2/10\n",
      "2622/2622 [==============================] - 57s 22ms/step - loss: 0.2943 - accuracy: 0.8729 - val_loss: 0.3535 - val_accuracy: 0.8468\n",
      "Epoch 3/10\n",
      "2622/2622 [==============================] - 58s 22ms/step - loss: 0.2693 - accuracy: 0.8815 - val_loss: 0.3772 - val_accuracy: 0.8485\n",
      "Epoch 4/10\n",
      "2622/2622 [==============================] - 58s 22ms/step - loss: 0.2482 - accuracy: 0.8883 - val_loss: 0.4043 - val_accuracy: 0.8463\n",
      "Epoch 5/10\n",
      "2622/2622 [==============================] - 57s 22ms/step - loss: 0.2308 - accuracy: 0.8938 - val_loss: 0.4090 - val_accuracy: 0.8428\n",
      "Epoch 6/10\n",
      "2622/2622 [==============================] - 58s 22ms/step - loss: 0.2161 - accuracy: 0.8987 - val_loss: 0.4235 - val_accuracy: 0.8436\n",
      "Epoch 7/10\n",
      "2622/2622 [==============================] - 58s 22ms/step - loss: 0.2008 - accuracy: 0.9031 - val_loss: 0.4636 - val_accuracy: 0.8267\n",
      "Epoch 8/10\n",
      "2622/2622 [==============================] - 58s 22ms/step - loss: 0.1905 - accuracy: 0.9086 - val_loss: 0.4932 - val_accuracy: 0.8408\n",
      "Epoch 9/10\n",
      "2622/2622 [==============================] - 58s 22ms/step - loss: 0.1789 - accuracy: 0.9139 - val_loss: 0.5054 - val_accuracy: 0.8312\n",
      "Epoch 10/10\n",
      "2622/2622 [==============================] - 57s 22ms/step - loss: 0.1720 - accuracy: 0.9200 - val_loss: 0.5196 - val_accuracy: 0.8207\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_basic = model.fit(train_input, train_target, epochs=10, validation_data=(val_input, val_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d21d6fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1106/1106 - 5s - loss: 0.5180 - accuracy: 0.8211\n",
      "[0.5179717540740967, 0.8210883140563965]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test,  y_test, verbose=2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f3b9088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "재밌어요. 추천합니다.\n",
      "긍정적인 리뷰일 확률 : 0.9479865431785583\n",
      "개쓰레기\n",
      "긍정적인 리뷰일 확률 : 0.18324002623558044\n",
      "이딴 거 볼 시간에 다른 거 보세요\n",
      "긍정적인 리뷰일 확률 : 0.010250389575958252\n",
      "정말 별로네요 제 마음의 별로\n",
      "긍정적인 리뷰일 확률 : 0.015339791774749756\n"
     ]
    }
   ],
   "source": [
    "my_corpus = ['재밌어요. 추천합니다.']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(f\"{my_corpus[0]}\\n긍정적인 리뷰일 확률 : {float(model.predict(tensor))}\")\n",
    "\n",
    "my_corpus = ['개쓰레기']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(f\"{my_corpus[0]}\\n긍정적인 리뷰일 확률 : {float(model.predict(tensor))}\")\n",
    "\n",
    "my_corpus = ['이딴 거 볼 시간에 다른 거 보세요']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(f\"{my_corpus[0]}\\n긍정적인 리뷰일 확률 : {float(model.predict(tensor))}\")\n",
    "\n",
    "my_corpus = ['정말 별로네요 제 마음의 별로']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(f\"{my_corpus[0]}\\n긍정적인 리뷰일 확률 : {float(model.predict(tensor))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e8ad03",
   "metadata": {},
   "source": [
    "## Mecab 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01f739c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_X)\n",
    "    \n",
    "    words = np.concatenate(X_train).tolist() #중복 제거\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['', '', '', ''] + [key for key, _ in counter]   \n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "        \n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index[''] for word in wordlist]\n",
    "        \n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "        \n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "    \n",
    "x_train, y_train, x_test, y_test, word_to_index = load_data(train, test, 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cfd59bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "516d2f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b336ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<BOS>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2  # unknown\n",
    "word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "index_to_word[0] = \"<PAD>\"\n",
    "index_to_word[1] = \"<BOS>\"\n",
    "index_to_word[2] = \"<UNK>\"\n",
    "index_to_word[3] = \"<UNUSED>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b91e326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 74, 919, 4, 4, 39, 228, 20, 33, 748]\n",
      "라벨:  0\n",
      "1번째 리뷰 문장 길이:  10\n",
      "2번째 리뷰 문장 길이:  17\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])  # 1번째 리뷰데이터\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨\n",
    "print('1번째 리뷰 문장 길이: ', len(x_train[0]))\n",
    "print('2번째 리뷰 문장 길이: ', len(x_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9dc7262b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  15.96940191154864\n",
      "문장길이 최대 :  116\n",
      "문장길이 표준편차 :  12.843571191092\n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4a1db971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146182, 41)\n"
     ]
    }
   ],
   "source": [
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70d6efe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 8)                 800       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 160,881\n",
      "Trainable params: 160,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원 수 (변경 가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model2 = keras.Sequential()\n",
    "model2.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model2.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model2.add(keras.layers.Dense(8, activation='relu'))\n",
    "model2.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d6ff8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136182, 41)\n",
      "(136182,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 10000건 분리\n",
    "x_val = x_train[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_x_train = x_train[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c4528bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "266/266 [==============================] - 13s 42ms/step - loss: 0.5132 - accuracy: 0.7696 - val_loss: 0.3724 - val_accuracy: 0.8412\n",
      "Epoch 2/10\n",
      "266/266 [==============================] - 11s 41ms/step - loss: 0.3477 - accuracy: 0.8546 - val_loss: 0.3443 - val_accuracy: 0.8517\n",
      "Epoch 3/10\n",
      "266/266 [==============================] - 11s 40ms/step - loss: 0.3201 - accuracy: 0.8660 - val_loss: 0.3387 - val_accuracy: 0.8543\n",
      "Epoch 4/10\n",
      "266/266 [==============================] - 11s 40ms/step - loss: 0.3062 - accuracy: 0.8732 - val_loss: 0.3400 - val_accuracy: 0.8545\n",
      "Epoch 5/10\n",
      "266/266 [==============================] - 11s 40ms/step - loss: 0.2957 - accuracy: 0.8777 - val_loss: 0.3404 - val_accuracy: 0.8552\n",
      "Epoch 6/10\n",
      "266/266 [==============================] - 11s 41ms/step - loss: 0.2862 - accuracy: 0.8823 - val_loss: 0.3464 - val_accuracy: 0.8533\n",
      "Epoch 7/10\n",
      "266/266 [==============================] - 11s 41ms/step - loss: 0.2763 - accuracy: 0.8873 - val_loss: 0.3486 - val_accuracy: 0.8531\n",
      "Epoch 8/10\n",
      "266/266 [==============================] - 11s 40ms/step - loss: 0.2675 - accuracy: 0.8914 - val_loss: 0.3532 - val_accuracy: 0.8542\n",
      "Epoch 9/10\n",
      "266/266 [==============================] - 11s 40ms/step - loss: 0.2586 - accuracy: 0.8959 - val_loss: 0.3557 - val_accuracy: 0.8540\n",
      "Epoch 10/10\n",
      "266/266 [==============================] - 11s 41ms/step - loss: 0.2505 - accuracy: 0.8993 - val_loss: 0.3635 - val_accuracy: 0.8536\n"
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history_model2 = model2.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d76ccb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 7s - loss: 0.3752 - accuracy: 0.8444\n",
      "[0.3751542270183563, 0.8443965315818787]\n"
     ]
    }
   ],
   "source": [
    "result = model2.evaluate(x_test,  y_test, verbose=2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ecb99ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "재밌어요. 추천합니다.\n",
      "긍정적인 리뷰일 확률 : 0.29685544967651367\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd883541790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "개쓰레기\n",
      "긍정적인 리뷰일 확률 : 0.4468419551849365\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd883541790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "이딴 거 볼 시간에 다른 거 보세요\n",
      "긍정적인 리뷰일 확률 : 0.8276686668395996\n",
      "정말 별로네요 제 마음의 별로\n",
      "긍정적인 리뷰일 확률 : 0.08203449845314026\n"
     ]
    }
   ],
   "source": [
    "my_corpus = ['재밌어요. 추천합니다.']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(f\"{my_corpus[0]}\\n긍정적인 리뷰일 확률 : {float(model2.predict(tensor))}\")\n",
    "\n",
    "my_corpus = ['개쓰레기']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(f\"{my_corpus[0]}\\n긍정적인 리뷰일 확률 : {float(model2.predict(tensor))}\")\n",
    "\n",
    "my_corpus = ['이딴 거 볼 시간에 다른 거 보세요']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(f\"{my_corpus[0]}\\n긍정적인 리뷰일 확률 : {float(model2.predict(tensor))}\")\n",
    "\n",
    "my_corpus = ['정말 별로네요 제 마음의 별로']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(f\"{my_corpus[0]}\\n긍정적인 리뷰일 확률 : {float(model2.predict(tensor))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b7709cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 8)                 800       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 160,881\n",
      "Trainable params: 160,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기\n",
    "word_vector_dim = 16  # 워드 벡터의 차원 수 (변경 가능한 하이퍼파라미터)\n",
    "\n",
    "model3 = keras.Sequential()\n",
    "model3.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model3.add(keras.layers.LSTM(8))   \n",
    "model3.add(keras.layers.Dense(8, activation='relu'))\n",
    "model3.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad4eebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "2622/2622 [==============================] - 62s 23ms/step - loss: 0.4868 - accuracy: 0.7559 - val_loss: 0.3700 - val_accuracy: 0.8384\n",
      "Epoch 2/7\n",
      "2622/2622 [==============================] - 59s 23ms/step - loss: 0.3389 - accuracy: 0.8530 - val_loss: 0.3554 - val_accuracy: 0.8447\n",
      "Epoch 3/7\n",
      "2622/2622 [==============================] - 60s 23ms/step - loss: 0.3057 - accuracy: 0.8662 - val_loss: 0.3557 - val_accuracy: 0.8479\n",
      "Epoch 4/7\n",
      "2622/2622 [==============================] - 59s 23ms/step - loss: 0.2813 - accuracy: 0.8761 - val_loss: 0.3768 - val_accuracy: 0.8463\n",
      "Epoch 5/7\n",
      "2622/2622 [==============================] - 59s 23ms/step - loss: 0.2603 - accuracy: 0.8852 - val_loss: 0.3814 - val_accuracy: 0.8458\n",
      "Epoch 6/7\n",
      "2622/2622 [==============================] - 60s 23ms/step - loss: 0.2426 - accuracy: 0.8917 - val_loss: 0.4043 - val_accuracy: 0.8409\n",
      "Epoch 7/7\n",
      "2622/2622 [==============================] - 60s 23ms/step - loss: 0.2285 - accuracy: 0.8970 - val_loss: 0.4107 - val_accuracy: 0.8383\n"
     ]
    }
   ],
   "source": [
    "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_3 = model3.fit(train_input, train_target, epochs=7, validation_data=(val_input, val_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5eef2a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 8s - loss: 0.8088 - accuracy: 0.5093\n",
      "[0.8087736368179321, 0.5093069076538086]\n"
     ]
    }
   ],
   "source": [
    "result = model3.evaluate(x_test,  y_test, verbose=2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9fb89888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "재밌어요. 추천합니다.\n",
      "긍정적인 리뷰일 확률 : 0.8517996072769165\n",
      "개쓰레기\n",
      "긍정적인 리뷰일 확률 : 0.24405452609062195\n",
      "이딴 거 볼 시간에 다른 거 보세요\n",
      "긍정적인 리뷰일 확률 : 0.04297676682472229\n",
      "정말 별로네요 제 마음의 별로\n",
      "긍정적인 리뷰일 확률 : 0.15714338421821594\n"
     ]
    }
   ],
   "source": [
    "my_corpus = ['재밌어요. 추천합니다.']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(f\"{my_corpus[0]}\\n긍정적인 리뷰일 확률 : {float(model3.predict(tensor))}\")\n",
    "\n",
    "my_corpus = ['개쓰레기']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(f\"{my_corpus[0]}\\n긍정적인 리뷰일 확률 : {float(model3.predict(tensor))}\")\n",
    "\n",
    "my_corpus = ['이딴 거 볼 시간에 다른 거 보세요']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(f\"{my_corpus[0]}\\n긍정적인 리뷰일 확률 : {float(model3.predict(tensor))}\")\n",
    "\n",
    "my_corpus = ['정말 별로네요 제 마음의 별로']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(f\"{my_corpus[0]}\\n긍정적인 리뷰일 확률 : {float(model3.predict(tensor))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dbc3de80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "진짜 너무 재밌어요\n",
      "긍정적인 리뷰일 확률 : 0.5823915600776672\n",
      "감독 천재인 듯\n",
      "긍정적인 리뷰일 확률 : 0.5060532689094543\n",
      "진짜 너무 재미없어요\n",
      "긍정적인 리뷰일 확률 : 0.1647377610206604\n",
      "이딴 거 만드는 데 쓴 돈이 아깝다\n",
      "긍정적인 리뷰일 확률 : 0.3770774006843567\n",
      "꼭 보세요 추천해요!\n",
      "긍정적인 리뷰일 확률 : 0.5352966785430908\n",
      "보지 마세요\n",
      "긍정적인 리뷰일 확률 : 0.44298607110977173\n"
     ]
    }
   ],
   "source": [
    "my_corpus = ['진짜 너무 재밌어요']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(f\"{my_corpus[0]}\\n긍정적인 리뷰일 확률 : {float(model2.predict(tensor))}\")\n",
    "\n",
    "my_corpus = ['감독 천재인 듯']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(f\"{my_corpus[0]}\\n긍정적인 리뷰일 확률 : {float(model2.predict(tensor))}\")\n",
    "\n",
    "my_corpus = ['진짜 너무 재미없어요']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(f\"{my_corpus[0]}\\n긍정적인 리뷰일 확률 : {float(model2.predict(tensor))}\")\n",
    "\n",
    "my_corpus = ['이딴 거 만드는 데 쓴 돈이 아깝다']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(f\"{my_corpus[0]}\\n긍정적인 리뷰일 확률 : {float(model2.predict(tensor))}\")\n",
    "\n",
    "my_corpus = ['꼭 보세요 추천해요!']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(f\"{my_corpus[0]}\\n긍정적인 리뷰일 확률 : {float(model2.predict(tensor))}\")\n",
    "\n",
    "my_corpus = ['보지 마세요']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(f\"{my_corpus[0]}\\n긍정적인 리뷰일 확률 : {float(model2.predict(tensor))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136362d8",
   "metadata": {},
   "source": [
    "## 회고\n",
    "\n",
    "loss 값이 고착화되고 학습이 제대로 이루어지지 않는 문제가 있었지만 model.fit부분을 다시 작성하니 제대로 작동되었다. 구체적으로 어떤 문제가 있었던 건진 모르겠어서 관련 자료를 찾아봐야할 거 같다. \n",
    "\n",
    "||Model|Model2|Model3|\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|정확도|0.8211|0.8444|0.5093|\n",
    "|재밌어요. 추천합니다.|0.9479865431785583|0.29685544967651367|0.8517996072769165|\n",
    "|개쓰레기|0.18324002623558044|0.4468419551849365|0.24405452609062195\n",
    "|이딴 거 볼 시간에 다른 거 보세요|0.010250389575958252|0.8276686668395996|0.04297676682472229\n",
    "|정말 별로네요 제 마음의 별로|0.015339791774749756|0.08203449845314026|0.15714338421821594\n",
    "\n",
    " SentencePiece를 활용했을 때와 Mecab을 이용한 경우 모두 성능이 80퍼센트 이상으로 준수한 결과를 보였다. 하지만 예시 문장을 직접 넣어봤을 땐 SentencePiece를 이용했을 경우엔 언어유희를 사용한 경우 말고는 적합한 결과를 보여줬지만 Mecab을 이용한 모델의 경우는 긍정과 부정을 잘 구분하지 못했다. 모델의 정확도와는 다른 결과를 보여줘 어떤 이유에서 이런 결과가 나온건지 생각해봐야 할 거 같다. 마지막으로 확인해보기 위해 위에 쓴 예시 외에 좀 더 의미가 분명한 문장으로 결과를 확인했더니 비교적 긍, 부정에 대한 확률이 구분되는 것을 볼 수 있었다. 하지만 여전히 그 수치 자체가 확실하게 높거나 낮진 않았다. Model3는 epoch, 단어 수에 변화를 줬는데 정확도가 낮게 나왔다. 그런데 예시 문장으로 결과를 확인했을 땐 Model과 마찬가지로 언어유희 외에는 적합하게 구분해내는 모습을 보였다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
